---
title: "SOC6707 Intermediate Data Analysis"
author: "Monica Alexander"
date: "Week 10: Interactions, Polytomous outcomes"
output: 
  beamer_presentation:
    slide_level: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, size = '\tiny')
```

```{r}
library(tidyverse)
library(here)
```




## Notes

- Assignment 3
- Research project analysis
- Plan for remaining weeks
    + today: multinomial
    + next week: miscellaneous research design
    + week 12: presentations



# Polytomous outcomes

## Polytomous outcomes

- So far we have only considered continuous and binary response variables, but what if we are interested in modeling a polytomous response variable as a function of continuous and/or categorical explanatory variables?
- A polytomous response variable is a variable that takes on one of $j>2$ possible values representing membership in one of $j>2$ different groups or categories. Examples:
    + Self-reported health
    + Voted Liberal, Conservative, NDP, Greens
    + Cause of death
- Polytomous response variables can be ordered or not, and can be modeled in several different ways
- Here I will focus on **multinomial logistic regression**

## Multinomial response

- A multinomial variable is a particular type of polytomous variable where the $j>2$ different groups or categories are not ordered
- Example: cause of infant death in the US. Here's what the dataset looks like:


```{r}
d <- read_rds(here("data", "infant.RDS"))
d <- d %>% 
  mutate(neo_death = ifelse(aged<=28, 1, 0),
         cod_group = case_when(
           str_starts(cod, "peri") ~ "perinatal",
           cod %in% c("other", "unknown") ~ "other/unknown",
           cod %in% c("sids", "maltreatment", "infection") ~ "exogenous",
           cod %in% c("resp", "heart") ~ "respiratory/heart",
           TRUE ~ "congenital malformations"
         ),
         preterm = ifelse(gest<37, 1, 0)) %>% 
  filter(gest<99, !is.na(mom_age_group))
infant <- d %>% select(race, mom_age, gest, preterm, cod_group)
head(infant) %>% kableExtra::kable()
```

## Cause of infant death

```{r}
infant %>% 
  group_by(race, cod_group) %>%
  tally() %>% 
  group_by(race) %>% 
  mutate(prop = n/sum(n)) %>%
  mutate(cod_group = fct_reorder(cod_group, -prop)) %>% 
  ggplot(aes(cod_group, prop, fill  = race)) + geom_bar(stat = "identity", position = 'dodge') + 
  labs(title = "Proportion of infant deaths by cause", x = "cause", y = "proportion") + 
  theme_bw() + 
  scale_fill_brewer(palette = "Set1")
```

## Multinomial distribution

- Now $Y_i$ make take one of several discrete values, $1,2,\dots, J$.
- Now the probability is 

$$
\pi_{ij} = Pr(Y_i=j)
$$
with
$$
\sum_j \pi_{ij} = 1
$$

- Note that this is an extension of the binomial distribution (for binary variables), which is the same thing, just with $J = 2$
- As such we can model multinomial outcomes in much the same way, using multinomial logistic regression

## Multinomial logistic regression

- Multinomial logistic regression is a model for the conditional probability that a multinomial response variable is equal to $j$ given a set of explanatory variables
- The MLRM can be expressed as
\footnotesize
$$
\log \left(\frac{P\left(Y_{i}=j \mid X_{i 1}, \ldots, X_{i k}\right)}{P\left(Y_{i}=1 \mid X_{i 1}, \ldots, X_{i k}\right)}\right)=\eta_{j i}=\beta_{j 0}+\beta_{j 1} X_{i 1}+\cdots+\beta_{j k} X_{i k} \quad \text { for } j=1, \ldots, J
$$
\normalsize
where $\log \left(\frac{P\left(Y_{i}=j \mid X_{i 1}, \ldots, X_{i k}\right)}{P\left(Y_{i}=1 \mid X_{i 1}, \ldots, X_{i k}\right)}\right)$ is known as the "log odds of response
category 'j' versus response category 1" and $\beta_{jk}$ are a set of unknown parameters subject to the constraint that $\beta_{1k} = 0$ for all $k$.

## Multinomial logistic regression

\footnotesize
$$
\log \left(\frac{P\left(Y_{i}=j \mid X_{i 1}, \ldots, X_{i k}\right)}{P\left(Y_{i}=1 \mid X_{i 1}, \ldots, X_{i k}\right)}\right)=\eta_{j i}=\beta_{j 0}+\beta_{j 1} X_{i 1}+\cdots+\beta_{j k} X_{i k} \quad \text { for } j=1, \ldots, J
$$

\normalsize
what is category 1?

- Doesn't really matter what it is
- R will by default choose (what)?
- But can change it to be what you want by re-leveling factors

## Multinomial logistic regression

Because the logit link function is invertible, we can also express the MLRM as an inverse logit function:
$$
\begin{aligned}
P\left(Y_{i}=j \mid X_{i 1}, \ldots, X_{i k}\right) &=\frac{\exp \left(\eta_{j i}\right)}{\sum_{j} \exp \left(\eta_{j i}\right)} \\
&=\frac{\exp \left(\beta_{j 0}+\beta_{j 1} X_{i 1}+\cdots+\beta_{j k} X_{i k}\right)}{\sum_{j} \exp \left(\beta_{j 0}+\beta_{j 1} X_{i 1}+\cdots+\beta_{j k} X_{i k}\right)}
\end{aligned}
$$

## Multinomial logistic regression

More specifically, we can express the conditional probabilities as follows:

$$
\begin{array}{c}
P\left(Y_{i}=1 \mid X_{i 1}, \ldots, X_{i k}\right)=\frac{\exp \left(\eta_{1 i}\right)}{\sum_{j} \exp \left(\eta_{j i}\right)}=\frac{1}{1+\exp \left(\eta_{2 i}\right)+\cdots+\exp \left(\eta_{J i}\right)} \\
P\left(Y_{i}=2 \mid X_{i 1}, \ldots, X_{i k}\right)=\frac{\exp \left(\eta_{2 i}\right)}{\sum_{j} \exp \left(\eta_{j i}\right)}=\frac{\exp \left(\eta_{2 i}\right)}{1+\exp \left(\eta_{2 i}\right)+\cdots+\exp \left(\eta_{J i}\right)} \\
\quad \vdots \\
P\left(Y_{i}=J \mid X_{i 1}, \ldots, X_{i k}\right)=\frac{\exp \left(\eta_{J i}\right)}{\sum_{j} \exp \left(\eta_{j i}\right)}=\frac{\exp \left(\eta_{J i}\right)}{1+\exp \left(\eta_{2 i}\right)+\cdots+\exp \left(\eta_{J i}\right)}
\end{array}
$$

## Interpretation

What is the parameter $\beta_{j1}$ for $j>1$?

\footnotesize
$$
\begin{array}{l}
\log \left(\frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*}+1, X_{i 2}=x_{2}^{*}, \ldots, X_{i k}=x_{k}^{*}\right)}{P\left(Y_{i}=1 \mid X_{i 1}=x_{1}^{*}+1, X_{i 2}=x_{2}^{*}, \ldots, X_{i k}=x_{k}^{*}\right)}\right)-\log \left(\frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*}, X_{i 2}=x_{2}^{*}, \ldots, X_{i k}=x_{k}^{*}\right)}{P\left(Y_{i}=1 \mid X_{i 1}=x_{1}^{*}, X_{i 2}=x_{2}^{*}, \ldots, X_{i k}=x_{k}^{*}\right)}\right) \\
\quad=\left(\beta_{j 0}+\beta_{j 1}\left(x_{1}^{*}+1\right)+\beta_{j 2} x_{2}^{*}+\cdots+\beta_{j k} x_{k}^{*}\right)-\left(\beta_{j 0}+\beta_{j 1} x_{1}^{*}+\beta_{j 2} x_{2}^{*}+\cdots+\beta_{j k} x_{k}^{*}\right) \\
\quad=\beta_{j 1} \\
\end{array}
$$

\normalsize
$\beta_{j1}$ is a log odds ratio that gives the change in the log odds that $Y_i$ is equal to $j$ rather than 1 associated with a unit increase in $X_{i1}$, holding other explanatory variables constant.

## Interpretation

What is $\exp(\beta_{j1})$?

\footnotesize

$$
\begin{aligned}
\exp \left(\beta_{j 1}\right) &=\exp \left(\log \left(\frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*}+1,\ldots\right)}{P\left(Y_{i}=1 \mid X_{i 1}=x_{1}^{*}+1, \ldots\right)} / \frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*}, \ldots, \right)}{P\left(Y_{i}=1 \mid X_{i 1}=x_{1}^{*},  \ldots \right)}\right)\right) \\
&=\frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*}+1, \ldots\right)}{P\left(Y_{i}=1 \mid X_{i 1}=x_{1}^{*}+1, \ldots \right)} / \frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*},  \ldots\right)}{P\left(Y_{i}=1 \mid X_{i 1}=x_{1}^{*}, \ldots\right)}
\end{aligned}
$$

\normalsize
$\exp(\beta_{j1})$ is the odds ratio that gives the multiplicative change in the odds that $Y_i$ is equal to $j$ rather than 1 associated with a unit increase in $X_{i1}$, holding other explanatory variables constant.

## Comparing other response categories

The preceding calculations concerned the contrast between response category $j$ and the baseline category 1, but they are easily extended to contrasts between any two categories $j$ and $j'$

Specifically, the log odds ratio that $Y_i$ is equal to $j$ rather than $j'$ associated with a unit increase in $X_{ik}$, holding other variables constant, is

\footnotesize
$$
\log \left(\frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*}+1 \ldots \right)}{P\left(Y_{i}=j^{\prime} \mid X_{i 1}=x_{1}^{*}+1,   \ldots \right)} / \frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*},   \ldots \right)}{P\left(Y_{i}=j^{\prime} \mid X_{i 1}=x_{1}^{*},   \ldots \right)}\right)=\beta_{j k}-\beta_{j^{\prime} k}
$$

and the corresponding odds ratio is

$$
\frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*}+1,   \ldots \right)}{P\left(Y_{i}=j^{\prime} \mid X_{i 1}=x_{1}^{*}+1,   \ldots \right)} / \frac{P\left(Y_{i}=j \mid X_{i 1}=x_{1}^{*},   \ldots \right)}{P\left(Y_{i}=j^{\prime} \mid X_{i 1}=x_{1}^{*},   \ldots \right)}=\exp \left(\beta_{j k}-\beta_{j^{\prime} k}\right)
$$

## General interpretations: take-away

Lots of symbols, but:

- interpretation of coefficients is direct extension of logistic
    + instead of "odds of yes versus no" it's "odds of thing outcome happening versus another outcome happening"
- so e.g. instead of "odds of dying versus not" it's "odds of dying from exogenous causes versus perinatal causes"

Maybe the trickiest bit is to get everything into the right format to run the regression

- We had data in long format, but we need summaries in wide format


## Example
Get data in wide format. Firstly, get the counts by covariate groups:

\tiny
```{r, echo = TRUE}
infant_counts <- infant %>% 
  group_by(race, mom_age, gest, preterm, cod_group) %>% 
  tally(name = "deaths")
infant_counts
```

## Example

Now get in wide format

\tiny
```{r, echo = TRUE}
infant_wide <- infant_counts %>% 
  pivot_wider(names_from = cod_group, values_from = deaths) %>% 
  mutate_all(.funs = funs(ifelse(is.na(.), 0, .)))
head(infant_wide)
```

## Example

Create outcome $Y$ which is a vector of cause-specific deaths

\tiny
```{r, echo=TRUE}
infant_wide$Y <- as.matrix(infant_wide[,c("perinatal",
                                          "exogenous",
                                          "congenital malformations", 
                                          "respiratory/heart", "other/unknown")])
head(infant_wide$Y)
```

## Example

\tiny
```{r, echo=TRUE}
library(nnet)
mod_mn <- multinom(Y ~ race+ mom_age+ preterm, data = infant_wide)
summary(mod_mn)
```
## Some interpretations

\tiny
```{r, echo = TRUE}
coef(mod_mn)
exp(coef(mod_mn))
```

\footnotesize
- The odds of exogenous causes compared to perinatal causes for NHW babies is 9% more than NHB babies, holding everything else constant
- The odds of respiratory/heart causes compared to perinatal causes for preterm babies is 90% less than for non-preterm babies, holding everything else constant
- The odds of respiratory/heart causes compared to congenital malformations for preterm babies is $\exp(-2.25 + 2.42) = 1.18$ times (or 18% more) than for non-preterm babies, holding everything else constant

## Predicted probabilities

- Now let's convert some of these coefficients into predicted probabilities
- We age, race, and preterm as covariates
- Easiest to pick an age (hold the continuous variable constant at a particular value) and then get predicted probabilities for all other groups
- Create a new tibble with all the groups we want predicted probabilities for:

\tiny
```{r, echo = TRUE}
predict_df <- tibble(race = rep(c("NHW", "NHB"), each = 2),
       mom_age = 30,
       preterm = rep(c(0,1),2)) 
predict_df
```


## Predicted probabilities

Now get the predictions

\tiny
```{r, echo = TRUE}
preds <- as_tibble(predict(mod_mn, newdata = predict_df, type = 'probs'))
preds
```

And join these back onto the tibble:

```{r, echo = TRUE}
preds <- bind_cols(predict_df, preds)
preds
```



## Predicted probabilities

```{r}
preds %>% 
  pivot_longer(`perinatal`:`other/unknown`, names_to = "cod_group", values_to = "probability") %>% 
  mutate(preterm = ifelse(preterm==1, "pre-term", "full-term")) %>% 
  ggplot(aes(race, probability, fill = cod_group)) + 
  geom_bar(stat = "identity")+
  facet_grid(~preterm) +
  ggtitle("Predicted probabilities of infant death by race, prematurity and cause\nMothers aged 30")
```

## Summary

- Multinomial logistic regression is a natural extension of binomial logistic regression 
- Useful when you have categorical outcomes with more than 2 categories

 A few words on generalized linear models

- So far we've seen linear regression (continuous), logistic regression (binary), and multinomial regression (categorical)
- Notice that all models are of the form 

$$
g(E(Y_i)) = \beta_0 + \beta_1X_{i1} + \dots + \beta_kX_{ik}
$$
where $g(.)$ is some function.

- For linear regression $g(.)$ is the identity
- For logistic regression $g(.)$ is the logit function
- For multinomial regression $g(.)$ is the log of the ratios of probabilities 

## Generalized linear models

- These are all special cases of generalized linear models (GLM)
- With the appropriate link function $g(.)$, a whole range of variables can be modeled in a linear framework
- We've looked at outcome variables with Normal, Binomial and Multinomial distributions
- But variables from any exponential distribution (a special family of distributions) can be modeled using GLMs
- Other common examples include Poisson, Gamma, and Negative Binomial regression