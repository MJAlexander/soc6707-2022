---
title: "SOC6707: Intermediate Data Analysis"
author: "Monica Alexander"
date: "Week 3: Exploratory Data Analysis and Data Visualization"
output: 
  beamer_presentation:
    slide_level: 2
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, size = '\tiny')
```

## Announcement

- We will be online until 28 February. 

## Where are we at

- Interested in a population, take a sample, to make inferences about a population
- We can summarize variables in our sample using a number of different useful measures (mean, median, mode, SD, variance, range, IQR)
- The process of taking a sample introduces randomness
- We quantify randomness and uncertainty using probability measures

## Where are we at

- Probability distributions summarize the probability/likelihood of an event occurring (e.g. a variable is in a certain range)
- If we plot probability distributions, the probability of an event is the area under the curve
- Probability distributions can be either discrete (e.g. coin toss) or continuous (e.g. height)
- the Normal distribution is a special case of a continuous probability distribution
- the Standard Normal distribution has mean 0 and SD = 1
- Any variable that is normally distributed can be converted into a standard normal variable (Z-score)

## Where are we going


What we will cover today:

- Why do we care so much about the Normal distribution (or, sampling distributions and the Central Limit Theorem)

Then:

- What is Exploratory Data Analysis (EDA) and why do we do it?
- Steps of EDA
- Data visualization principles

Lab: getting data and reading it into R

# Sampling distributions and the central limit theorem

## Why do we care about the Normal distribution so much

- It's just one distribution
- It tends to over-simplify the real distribution of outcomes/variables

BUT it turns out that the distribution of summary statistics that we're interested in (e.g. means) tend towards being Normal 

- this is important for regression and inference

## Sampling distributions 

A **sampling distribution** is a probability distribution for a statistic based on repeated samples.

Say we are interested in taking a random sample of people's heights, $X$ and calculating the mean height for that sample. So our statistic of interest is the mean height, $\bar{X}$.


## Randomly sampling heights


We first take a random sample of 12 people and get the following heights

\tiny
```{r, echo = FALSE}
library(tidyverse)
heights_1 <- rnorm(12, mean = 170, sd = 8)
round(heights_1, 1)
```
\normalsize
The observed mean height of this sample is `r round(mean(heights_1),1)`.

We take a random sample of another 12 people and get the following heights:

\tiny
```{r, echo = FALSE}
heights_1 <- rnorm(12, mean = 170, sd = 8)
round(heights_1, 1)
```

\normalsize
The observed mean height of this sample is `r round(mean(heights_1),1)`.

## Randomly sampling heights

Say that I keep doing this process again and again and again, and end up with 100 observations of mean height. I can plot a histogram of these means:

```{r, echo = FALSE, fig.height = 5}

N <- 100 
heights_0 <- tibble()

for(i in 1:N){
  these_heights = tibble(sample = i, heights = rnorm(12, 170, 8))
  heights_0 <- bind_rows(heights_0, these_heights)
}

heights_0 %>% 
  group_by(sample) %>% 
  summarise(mean_height = mean(heights)) %>% 
  ggplot(aes(mean_height)) + geom_histogram(color = "navy", fill = "lightblue")

```

## Randomly sampling heights

Okay, what if I take 1000 samples. I plot a histogram of the means again

```{r, echo = FALSE, fig.height = 5}

N <- 1000
heights <- tibble()

for(i in 1:N){
  these_heights = tibble(sample = i, heights = rnorm(12, 170, 8))
  heights <- bind_rows(heights, these_heights)
}

heights %>% 
  group_by(sample) %>% 
  summarise(mean_height = mean(heights)) %>% 
  ggplot(aes(mean_height)) + geom_histogram(color = "navy", fill = "lightblue")

```
What do you notice?

## Randomly sampling heights

What about 10,000 samples?

```{r, echo = FALSE, fig.height = 5}

N <- 10000
heights_2 <- tibble()

for(i in 1:N){
  these_heights = tibble(sample = i, heights = rnorm(12, 170, 8))
  heights_2 <- bind_rows(heights_2, these_heights)
}

heights_2 %>% 
  group_by(sample) %>% 
  summarise(mean_height = mean(heights)) %>% 
  ggplot(aes(mean_height)) + geom_histogram(color = "navy", fill = "lightblue")

```

## The central limit theorem

The distribution of the sum (or mean) of a set of independent random variables will **tend towards** a normal distribution. 

- "tend towards" means as the number of observations of the sum or mean gets larger, the distribution will become more normal
- The central limit theorem holds even if the original variables themselves are not normally distributed. 


## The central limit theorem

For a random variable $X$ with $E(X) = \mu$ and $Var(X) = \sigma^2$, the central limit theorem results in the following distribution for the mean $\bar{X}$:

$$
\bar{X} \sim N(\mu, \frac{\sigma^2}{n})
$$

What does this mean?

- The mean $\bar{X}$ will be centered at the same value as $X$
- The variance of $\bar{X}$ depends on the variance of the original random variable $X$ and also the number of samples of the mean we have, $n$.

The quantity $\frac{\sigma}{\sqrt n}$ is also called the **standard error of the mean**.

## Sampling heights

Before, we were repeatedly taking a random sample of 12 heights. 

When we did this 100 times, we had 100 observations of $\bar{X}$. The mean was `r round(mean(heights_0$heights))` and the standard error of the mean was `r round(sd(heights_0$heights), 1)/100`.

When we did this 10000 times, we had 100 observations of $\bar{X}$. The mean was `r round(mean(heights_2$heights))` and the standard error of the mean was `r round(sd(heights_2$heights), 1)/10000`.


# Exploratory Data Analysis (EDA)

## What is EDA and why do we do it?

Before we even do any sort of statistical inference, we need to understand the main characteristics of our dataset. 

- Helps to identify any potential issues or surprising things about our data
- Helps to check / explore / refine research questions

## What is EDA and why do we do it?

EDA is all about asking: 

- What types of variables do we have?
- Do we have a complete dataset, or do we have missing data or observations?
- If we have missing data, is it missing equally across observations of different types or concentrated in particular groups?
- Are there any obvious outliers or strange data points?
- What do the data 'look' like? 
    + summary measures, measures of centrality, spread
    + Visualizing the data through plots and tables
    
## Steps of EDA

1. Become familiar with size of data set (number of observations and variables available)
2. What kinds of variables are available
3. For the variables that I'm interested in, are there any missing values or other issues?
4. What does the distribution/frequency of observations look like for the variables I'm interested in? (summary measures, tables and graphs)

## Example: TTC subway delays in 2019

- Data on TTC subway delay times by station and day available from the Open Data Toronto website: https://open.toronto.ca/

```{r, echo = FALSE}
library(tidyverse)
library(here)
delay_2019 <- read_csv(here("data/ttc_delays_2019.csv"))
```

\includegraphics{../fig/ttc.jpg}


## Get familiar with dataset

\tiny
```{r}
delay_2019
```

## Get familiar with dataset

Dimensions (number of rows x number of columns)

\tiny

```{r}
dim(delay_2019)
```

Variable names

\tiny

```{r}
colnames(delay_2019)
```

## The `summary` function is useful for a quick overview

\tiny
```{r}
summary(delay_2019)
```

## Research question?

- What are some good potential research questions with this dataset?

## Sanity checks

We need to check variables should be what they say they are. If they aren't, the natural next question is to what to do with issues (recode? remove?)

E.g. check days of week make sense with the `unique` function

\tiny

```{r}
delay_2019 %>% 
  select(day) %>% 
  unique()
```

## Sanity checks

Check lines: oh no. some issues here. Some have obvious recodes, others, not so much. 
\tiny
```{r}
delay_2019 %>% 
  select(line) %>% 
  unique() %>% 
  pull() # turn into a vector for better display
```
## Data issues

How bad is the mislabeling of lines? look at frequency of cases

NOTE! New very important function: `group_by` 

\tiny

```{r}
delay_2019 %>% 
  group_by(line) %>% # group by line label
  tally() %>% # count the number of occurrences
  arrange(-n) # arrange in descending order
```
## Missing values

\tiny

```{r}
delay_2019 %>% 
  summarise_all(.funs = funs(sum(is.na(.))))
```


<!-- ## Or use the `skimr` package -->

<!-- \tiny  -->

<!-- ```{r, eval = FALSE} -->
<!-- library(skimr) # install using install.packages("skimr") -->
<!-- skim(delay_2019) -->
<!-- ``` -->

<!-- \normalsize -->
<!-- (Show this in lecture) -->

```{r, echo = FALSE}
library(janitor)
delay_2019 <- delay_2019 %>% distinct()
delay_2019 <- delay_2019 %>% filter(line %in% c("BD", "YU", "SHP", "SRT")) 
```


## Summary statistics

Most interested in delay minutes, which is the `min_delay` variable

\tiny

```{r}
delay_2019 %>% 
  summarize(n_obs = n(), 
            mean_delay = mean(min_delay),
            median_delay = median(min_delay),
            range_delay = max(min_delay) - min(min_delay),
            iqr_delay = IQR(min_delay))
```

## Summary statistics

Probably more interesting to do these summaries by line (**stratify** by line); easy extension with the `group_by` function

\tiny

```{r}
delay_2019 %>% 
  group_by(line) %>% 
  summarize(n_obs = n(), 
            mean_delay = mean(min_delay),
            median_delay = median(min_delay),
            range_delay = max(min_delay) - min(min_delay),
            iqr_delay = IQR(min_delay))
```


## Summaries

Could also stratify by reason for delay

\tiny

```{r}
delay_2019 %>% 
  group_by(code_desc) %>% 
  summarize(n_obs = n(), 
            mean_delay = mean(min_delay),
            median_delay = median(min_delay),
            range_delay = max(min_delay) - min(min_delay),
            iqr_delay = IQR(min_delay)) %>% 
  arrange(-n_obs)
```

## Summaries

Arrange by mean delay time

\tiny

```{r}
delay_2019 %>% 
  group_by(code_desc) %>% 
  summarize(n_obs = n(), 
            mean_delay = mean(min_delay),
            median_delay = median(min_delay),
            range_delay = max(min_delay) - min(min_delay),
            iqr_delay = IQR(min_delay)) %>% 
  arrange(-mean_delay) 
```
## EDA: summary so far

- There's no one checklist of things to looks at, depends on your data and research question
- Get familiar with your dataset
- Check for missing values, and that existing values make sense
- Summary statistics depend on your research question of interest
    + stratifying (`group_by`) by important characteristics often useful 


# Data visualization

## Plot your data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

- We started to compute some summary statistics above, and showed how summaries can be calculated by group and arranged in different ways to get a sense of differences across groups
- However, graphing/plotting your data is usually the best way to visualize patterns, trends, outliers, issues and other surprising points
- The most appropriate types of graph for your data depends on:
    + the type of variable you are interested in (quantitative or qualitative/categorical)
    + your research questions


## Plot your data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

- Before you start to do any statistical analysis, you should always plot your data
- Data visualization is a key part of EDA and essential in understanding the assumptions and outcomes of your eventual statistical analysis

## Plot your data!!!!!!!!!!!!!!!!!!!!!!!!!!!

Here's a specific example. Imagine we have the following sets of datasets of (x,y) pairs

\tiny
```{r}
library(tidyverse)
library(datasauRus)
head(datasaurus_dozen)
```

##
How many observations?

\tiny
```{r}
datasaurus_dozen  %>%  count(dataset)
```
##

Do some summaries for each dataset

\tiny
```{r}
datasaurus_dozen %>% 
  group_by(dataset) %>% 
  summarise(mean_x = mean(x),
            mean_y = mean(y),
            correlation = cor(x,y))
```
## Aside: another summary measure

On the previous slide, x and y were negatively correlated.

**Correlation** is the statistical measure of the relationship between two variables. **Pearson's correlation coefficient**, $r_{xy}$ summarizes this relationship into one number. For an observation sample of two random variables $x_1, x_2, \dots, x_n$ and $y_1, y_2, \dots, y_n$, 

$$
r_{xy} = \frac{\sum^n_{i=1}(x_i - \bar x)(y_i - \bar y)}{\sqrt{\sum^n_{i=1}(x_i - \bar x)^2}\sqrt{\sum^n_{i=1}(y_i - \bar y)^2}}
$$

## Summaries are very similar

\tiny
```{r}
datasaurus_dozen %>% 
  group_by(dataset) %>% 
  summarise(mean_x = mean(x),
            mean_y = mean(y),
            correlation = cor(x,y))
```

## But now let's plot

\tiny
```{r, echo = FALSE}
datasaurus_dozen  %>% 
  filter(dataset %in% c("dino", "star", "away", "bullseye")) %>% 
  ggplot(aes(x=x, y=y, colour=dataset)) +
  geom_point() +
  theme_minimal() +
  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +
  labs(colour = "Dataset")
```



## Anscombe's quartet

This is a modern version of a famous plot ‘Anscombe’s Quartet.’ That plot conveys the same message about the importance of plotting the actual data and not relying on summary statistics. 

```{r, echo = FALSE, fig.width=6, fig.height=3}
datasets::anscombe %>% 
  select(x1:x4) %>% 
  pivot_longer(x1:x4) %>% 
  mutate(name = as.numeric(str_remove(name, "x"))) %>% 
  rename(dataset = name,
         x = value) %>% 
  group_by(dataset) %>% 
  mutate(i = 1:n()) %>% 
  left_join(datasets::anscombe %>% 
  select(y1:y4) %>% 
  pivot_longer(y1:y4) %>% 
  mutate(name = as.numeric(str_remove(name, "y"))) %>% 
  rename(dataset = name,
         y = value) %>% 
  group_by(dataset) %>% 
  mutate(i = 1:n())) %>% 
  ggplot(aes(x, y)) + geom_point() + facet_wrap(~dataset) + 
  geom_abline(intercept = 3, slope = 0.5, color = "royalblue", lwd = 1.2)+
  theme_bw(base_size = 14)
```


## 

\includegraphics{../fig/halloween.jpeg}


# What makes a good plot?

## The spiral of doom

\centering
\includegraphics[width = 0.6\textwidth]{../fig/spiral.png}



```{r, echo = FALSE}
library(tidyverse)
library(lubridate)
# Recreation: https://bydata.github.io/nyt-corona-spiral-chart/
owid_url <- "https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true"
country <- "United States"
covid <- read_csv(owid_url)
covid_cases <- covid %>% 
  filter(location == country) %>% 
  select(date, new_cases, new_cases_smoothed) %>% 
  arrange(date) %>% 
  # Add the dates before the 1st confirmed case
  add_row(date = as_date("2020-01-01"), new_cases = 0, new_cases_smoothed = 0,
          .before = 1) %>% 
  complete(date = seq(min(.$date), max(.$date), by = 1),
           fill = list(new_cases = 0, new_cases_smoothed = 0)) %>% 
  mutate(day_of_year = yday(date),
         year = year(date)
         )
```
## Line plot

```{r, echo = FALSE}
covid_cases %>% 
  ggplot(aes(date, new_cases_smoothed)) + 
  geom_line(color = "#D97C86", lwd = 1.2) +
  theme_minimal() + 
  labs(title = "New Covid-19 cases, United States", y = "")
```

## Area plot

```{r, echo = FALSE}
covid_cases %>% 
  ggplot(aes(date, new_cases_smoothed)) + 
  geom_area(fill = "#D97C86") +
  theme_minimal() + 
  labs(title = "New Covid-19 cases, United States", y = "")
```

## Ribbons

```{r, echo=FALSE}
covid_cases %>% 
  mutate(y = 0) %>% 
  ggplot(aes(date, y)) + 
  geom_line() + 
  geom_ribbon(aes(ymin = month(date) - new_cases_smoothed / 2, ymax = month(date) + new_cases_smoothed / 2), fill = "#F0C0C1", color = "#D97C86") + 
  geom_vline(xintercept = as_date("2021-01-01"), lty = 2, color = "grey28")+
  geom_vline(xintercept = as_date("2022-01-01"), lty = 2, color = "grey28")+
  geom_vline(xintercept = as_date("2020-01-01"), lty = 2, color = "grey28") + 
  geom_hline(yintercept = 0, lty = 1, color = "grey4") +
  theme_void()
```





<!-- ## Ribbons -->

<!-- ```{r, echo=FALSE} -->
<!-- covid_cases %>%  -->
<!--   mutate(y = 0) %>%  -->
<!--   ggplot(aes(date, y)) +  -->
<!--   geom_line() +  -->
<!--   geom_ribbon(aes(ymin = month(date) - new_cases_smoothed / 2, ymax = month(date) + new_cases_smoothed / 2), fill = "#F0C0C1", color = "#D97C86") +  -->
<!--    #geom_vline(xintercept = as_date("2021-01-01"), lty = 2, color = "grey28")+ -->
<!--    #geom_vline(xintercept = as_date("2022-01-01"), lty = 2, color = "grey28")+ -->
<!--    #geom_vline(xintercept = as_date("2020-01-01"), lty = 2, color = "grey28") +  -->
<!--   # geom_hline(yintercept = 0, lty = 1, color = "darkgray") + -->
<!--   theme_void()+ -->
<!--   coord_flip() -->
<!--   #geom_text(x=as_date("2020-01-01")+30, y=4.1*10^5, label="2020")+ -->
<!--   #geom_text(x=as_date("2021-01-01")+30, y=4.1*10^5, label="2021")+ -->
<!-- #geom_text(x=as_date("2022-01-01")+30, y=4.1*10^5, label="2022") -->
<!-- ``` -->




## What makes a good plot?

- Is the spiral the right graph to use?
- What does right mean?
- Does it effectively portray the information?
- Is it misleading?
- Is it easy to read?
- Is it memorable?


## Data visualization principles

- Choose the right graph
- Know your audience
- Emphasize important patterns without being misleading
- Clear, effective designs

## Choose the right graph

Choosing the right graph primarily depends on the type of variables that you are trying to visualize:

- Quantitative variables e.g. histograms, scatter plots
- Qualitative variables e.g. barcharts

Choose the graph based on the kind of data and the message to be conveyed.

- Do not use different graphs just for variety, as specific graphs convey certain types of information more effectively than others.
- If not required, do not use any chart — show only numbers.



## Pie charts 

```{r, echo = FALSE}
library(babynames)
db <- babynames %>% 
  mutate(starts_with = str_sub(name, 1, 1)) %>% 
  group_by(year,sex, starts_with) %>% 
  summarise(prop = sum(prop)) %>% 
  arrange(sex, year, -prop) %>% 
  filter(year==1990|year==2000|year==2010, sex=="F") %>% 
  mutate(starts_with_big = ifelse(starts_with %in% c("A", "K", "J", "S", "C", "M"), starts_with, "Other")) %>% 
  group_by(year, starts_with_big) %>% 
  summarise(prop = sum(prop))

par(mfrow = c(1, 2))
pie(db$prop[db$year==1990], 
    main = "Girl's names by starting letter, 1990", 
    col =rainbow(7), 
    labels = paste0(paste(db$starts_with_big[db$year==1990], round(db$prop[db$year==1990], 3)*100, sep = ", "), "%"))

pie(db$prop[db$year==2010], 
    main = "Girl's names by starting letter, 2010", 
    col =rainbow(7), 
    labels = paste0(paste(db$starts_with_big[db$year==2010], round(db$prop[db$year==2010], 3)*100, sep = ", "), "%"))
```

## Pie charts

```{r}
?pie
```

> Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.


## Alternative

```{r, echo = FALSE}
db %>% 
  filter(year!="2000") %>% 
  mutate(starts_with_big = fct_reorder(starts_with_big, prop)) %>% 
  ggplot(aes(starts_with_big, prop, fill = factor(year))) + 
  geom_bar(stat = 'identity', position = 'dodge') + 
  labs(title = "Girl's names starting letter, 1990 and 2010", y = "proportion", x = "letter") + 
  theme_bw() + 
  scale_fill_brewer(palette = "Set1", name = "year")
```


## Know your audience

Graphs can be used for 

- our own exploratory data analysis
- to convey a message to experts, 
- to help tell a story to a general audience. 

Make sure that the intended audience understands each element of the plot.

Examples: spiral plot, log scales

- Think of the color blind. In R, `viridis` and `brewer` palettes give colorblind-friendly options

## Emphasize important patterns without being misleading

> There is no such thing as information overload. There is only bad design. — Edward Tufte

- Eliminate distractions
- Highlight the essential
- Use color and text strategically
- Avoid pseudo-3D plots

## Highlight the essential

\centering
\includegraphics[width = 1.1\textwidth]{../fig/opioids.png}

Source: https://link.springer.com/article/10.1007/s11524-021-00573-8

# When to start the axis at zero?

\includegraphics{../fig/class2_8.jpeg}

\tiny
[Source](https://www.mediamatters.org/fox-news/fox-news-newest-dishonest-chart-immigration-enforcement)

## When to start the axis at zero?

\includegraphics{../fig/temp_1.png}


## When to start the axis at zero?

\includegraphics{../fig/temp_2.jpeg}

## When to include zeroes

- With bar plots, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are.
- With line plots or plots that use position, it is not neccessary to start the axis at zero (and could be misleading)

##

```{r, echo = FALSE}

country_indicators <- read_csv("../../data/country_indicators.csv")
p1 <- country_indicators %>% 
  filter(year==2010) %>% 
  ggplot(aes(region, life_expectancy)) + geom_boxplot() + coord_flip()+
  labs(title = "Life expectancy (years), 2010", x = "", y = "life expectancy")+
  theme_bw(base_size = 16)

country_indicators <- read_csv("../../data/country_indicators.csv")
p2 <- country_indicators %>% 
  filter(year==2010) %>% 
  ggplot(aes(region, life_expectancy)) + geom_boxplot() + coord_flip() + 
  ylim(c(0, 90)) +
  labs(title = "Life expectancy (years), 2010", x = "", y = "life expectancy")+
  theme_bw(base_size = 16)
  
p1
```


##

```{r, echo = FALSE}
p2+
  theme_bw(base_size = 16)
```


## Emphasize important patterns without being misleading

\includegraphics{../fig/white_house.jpeg}

## Clear, effective designs

\centering
\includegraphics[width = 0.65\textwidth]{../fig/e0_reversal.jpeg}



## Important types of graphs: next week

- Histograms
- Bar charts
- Boxplots
- Line plots
- Scatter plots 

## ...but data visualization need not be a graph

\centering
\footnotesize
Hobart, Tasmania, Australia
\includegraphics[width = 0.43\textwidth]{../fig/helen_scarf}\\

Toronto, Ontario, Canada
\includegraphics[width = 0.43\textwidth]{../fig/mon_scarf}

## Data ideas

- IPUMS: https://ipums.org/
- ICPSR: https://www.icpsr.umich.edu/web/pages/ICPSR/thematic-collections.html
- CHASS SDA: https://datacentre.chass.utoronto.ca/
- Toronto Open Data Portal: https://open.toronto.ca/ or use `opendatatoronto` R package (ask for code)
- UN WPP: https://population.un.org/wpp/
- NBER: https://www.nber.org/research/data?page=1&perPage=50



