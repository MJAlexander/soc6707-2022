---
title: "SOC6707 Intermediate Data Analysis"
author: "Monica Alexander"
date: "Week 2: Probability! Chance! Randomness!"
output: 
  beamer_presentation:
    slide_level: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, size = '\tiny')
library(tidyverse)
```


## Announcements

- Assignment 1 is up
- tidyverse, some plotting, some probabilities
- Also added a 'template' (helpful? maybe not?)
- Also due is research proposal
- Datasets you need are on Quercus
- Discussion boards are up


## What's the point of today

- Population --> Sample --> Population
- We are introducing randomness, but trying to make meaningful inferences despite this
- Need to know basic probability concepts 
- This helps us to talk about distributions of the statistical quantities we are interested in
    + e.g. population means, but also regression coefficients

## Random variables

From first week: A **random variable** is a variable whose values depend on the outcomes of a random process. 

Examples

- Flipping a coin four times and recording the number of heads
- Randomly sampling six people and recording their height
- A toddler randomly selecting a Lego car

<!-- ## Coin toss example -->

<!-- Imagine tossing a coin 4 times. Say we are interested in the number of heads that turns up. The observed outcomes are: -->

<!-- ```{r, echo = FALSE} -->
<!-- library(tidyverse) -->
<!-- set.seed(999) -->
<!-- tosses <- purrr::rbernoulli(4) -->
<!-- ifelse(tosses, "H", "T") -->
<!-- ``` -->

<!-- So the number of heads is 2. But we can toss it another 4 times. The second set of observed outcomes are -->

<!-- ```{r, echo = FALSE} -->
<!-- set.seed(176) -->
<!-- tosses <- purrr::rbernoulli(4) -->
<!-- ifelse(tosses, "H", "T") -->
<!-- ``` -->

<!-- So the number of heads is 1.  -->

<!-- The number of heads is a **random variable** that depends on the random process of flipping a coin. -->

<!-- ## Heights example -->

<!-- Say we are interested in heights of people in Canada. We take a random sample of 6 people. Their heights are (in cm) -->

<!-- ```{r, echo = FALSE} -->
<!-- set.seed(6) -->
<!-- round(rnorm(6, mean = 175, sd = 8), 2) -->
<!-- ``` -->

<!-- We sample another 6 people. Their heights are  -->

<!-- ```{r, echo = FALSE} -->
<!-- round(rnorm(6, mean = 175, sd = 8), 2) -->
<!-- ``` -->

<!-- So height is a random variable that depends on the random process of sampling the population -->

<!-- ## Notation -->

<!-- - Call our random variable of interest $X$ -->
<!--     + in coin example $X =$ number of heads -->
<!--     + in heights example $X=$ height -->
<!-- - After we observe values we denote these with lower case $x$ -->
<!--     + coin example $x = 2$ and $x = 1$  -->
<!--     + heights example $\{x_1 = 177.16, x_2 = 169.96, x_3 = 181.95, x_4 = 188.82, x_5 = 175.19, x_6 = 177.94\}$ etc -->


# Probability essentials

## Probability

- Based on our sample or other random process (as in the coin flipping or a toddler choosing Lego), we would like to make valid statements about the underlying population or quantity of interest
- Probability is one tool that will help us do that
- Probability is all about talking about the chance of something (an event happening or observing a particular thing)
- There is uncertainty associated with the event or observation, and probability helps us to quantify this

<!-- ## Definitions and notation -->

<!-- - **Experiment**: An experiment can be any process, in a laboratory or otherwise, where we can observe the result of a process and the result of that process is uncertain.  -->
<!-- - **Events**: outcomes of an experiment. Denote event $i$ as $A_i$, with $N$ possible events -->
<!-- - **Sample space**: A listing of all possible events  -->
<!-- - **Probability function**: a rule that assigns a value $P(A_i)$ to each event such that  -->
<!--     + $P(A_i)$ is greater than or equal to zero ($P(A_i) \geq 0$) -->
<!--     + $P(A_i)$ is less that or equal to one ($P(A_i) \leq 1$) -->
<!--     + the sum of all $P(A_i)$ is equal to one for a finite sample space. ($\sum_i^N P(A_i) =1$) -->
    
## Definitions 

- **Events**: things that can happen
    + what's an example of an event when flipping a coin once? Four times?
    + what's an example of an event of sampling six people's heights?
- **Probability function**: a rule that assigns a value $P(A)$ to each event $A$. We know
    + Probability is positive
    + Probability is at most 1
    + The sum of probablities of all possible events is 1

## Lego example

We have the following lego trains and cars:

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{../fig/lego_train1.jpeg}
\end{figure}

## Lego example

My son randomly draws out one vehicle 

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{../fig/lego_train2.jpeg}
\end{figure}

## Lego example

Let's define some events:

- A = "Choose a train"
- B = "Choose a vehicle that is blue"

What is $P(A)$? What is $P(B)$?

Probability is just counting!

## Probability is just counting!

<!-- ## Green blocks -->
<!-- \begin{figure} -->
<!-- \centering -->
<!-- \includegraphics[width = 0.6\textwidth]{../fig/numbers5} -->
<!-- \end{figure} -->

<!-- ## Numbers less than 5 -->
<!-- \begin{figure} -->
<!-- \centering -->
<!-- \includegraphics[width = 0.6\textwidth]{../fig/numbers6} -->
<!-- \end{figure} -->

<!-- ## Additive / Union rule -->

<!-- What is $P(A \text{ or } B)$? That is the probability that the block is green or has a number less than 5? -->

<!-- $$P(A \text{ or } B) = P(A) + P(B) - P(A \text{ and }B)$$ -->
<!-- Note that if $A$ and $B$ are **mutually exclusive** then they can't happen together so $P(A \text{ or } B) = P(A) + P(B)$. -->

<!-- ## Multiplicative / Intersection rule -->

<!-- What is $P(A \text{ and } B)$? That is the probability that the block is green and has a number less than 5? -->

<!-- $$P(A \text{ and } B) = P(A) \times P(B|A)$$ -->

<!-- - $P(B|A)$ is conditional probability i.e. the probability of B given that A is true -->
<!-- - In our example, the probability of observing a number less than 5 given the block is green -->

<!-- ## Independence -->

<!-- If two events $A$ and $B$ are independent, then $P(A)$ is not affected by the condition $B$, and vice versa, so we can say that $P(A|B = P(A)$ and likewise, $P(B|A) = P(B)$, so the multiplicative rule becomes -->


<!-- $$P(A \text{ and } B) = P(A) \times P(B)$$ -->

<!-- ## Complements -->

<!-- the complement of any event $A$ is the event [not $A$], i.e. the event that $A$ does not occur. It is denoted $A^c$. -->

<!-- ## Lego practice -->

<!-- Interpret and calculate the following -->

<!-- - $P(B|A)$ -->
<!-- - $P(A|B)$ -->
<!-- - $P(A^c)$ -->
<!-- - $P(A|B^c)$ -->

## Conditional probability

- The probability of something happening given we know something else
- $P(B|A)$ is conditional probability i.e. the probability of B given that A is true 
- Lego examples
    + what is $P(B|A)$?
    + what is the probability that the vehicle is a train given it has red wheels?
    + what is the probability that the vehicle is white given it is a car? 

## Conditional probability

Conditional probability is important for us 

- What's the probability that someone work's remotely given they work in finance (vs hospitality?)
- What's the probability that someone graduates college given their parent's did?

# Probability distributions

## Back to coin flipping example

- The process of tossing a coin four times qualifies as an experiment
- We can observe the outcome of each toss, and the outcome is uncertain. 
- Our random variable of interest was the number of heads

First, letâ€™s look at possibilities. On the first toss, we could observe an outcome of heads (H) or tails (T). On each of the remaining three tosses, we could observe an H or a T. Thus, the possibilities for four tosses can be enumerated as follows:

- HHHH, HHHT, HHTH, HHTT, HTHH, HTHT, HTTH, HTTT, THHH, THHT, THTH, THTT, TTHH, TTHT, TTTH, and TTTT. 

We can see that there are 16 different possible outcomes when listed as simple events. 

## Flipping a coin

We can enumerate these possible outcomes in a table with the associated probability and observed number of heads

\footnotesize

\begin{table}[]
\begin{tabular}{|l|r|r|}
\hline
event & probability & number of heads \\ \hline
HHHH  & 0.0625      & 4               \\ \hline
HHHT  & 0.0625      & 3               \\ \hline
HHTH  & 0.0625      & 3               \\ \hline
HHTT  & 0.0625      & 2               \\ \hline
HTHH  & 0.0625      & 3               \\ \hline
HTHT  & 0.0625      & 2               \\ \hline
HTTH  & 0.0625      & 2               \\ \hline
HTTT  & 0.0625      & 1               \\ \hline
THHH  & 0.0625      & 3               \\ \hline
THHT  & 0.0625      & 2               \\ \hline
THTH  & 0.0625      & 2               \\ \hline
THTT  & 0.0625      & 1               \\ \hline
TTHH  & 0.0625      & 2               \\ \hline
TTHT  & 0.0625      & 1               \\ \hline
TTTH  & 0.0625      & 1               \\ \hline
TTTT  & 0.0625      & 0               \\ \hline
\end{tabular}
\end{table}

## Flipping a coin

Using this we can work out different probabilities. e.g. probability of 3 heads

$P(X = 3) = P(HHHT \text{ or } HHTH \text{ or } HTHH \text{ or } THHH) = 4/16$

## Probability distribution for the number of heads

Given our RV of interest is the number of heads and that all events are mutually exclusive, we can summarize the table as

\begin{table}[]
\begin{tabular}{|r|r|}
\hline
\multicolumn{1}{|l|}{Number of heads (X)} & \multicolumn{1}{l|}{P(X)} \\ \hline
4                                         & 1/16                      \\ \hline
3                                         & 4/16                      \\ \hline
2                                         & 6/16                      \\ \hline
1                                         & 4/16                      \\ \hline
0                                         & 1/16                      \\ \hline
\end{tabular}
\end{table}

We have a **probability distribution** for the number of heads. That is, a rule or function that associates the probability of observing that particular value with each value of a random
variable. The probability distribution for a **discrete** RV (like # heads) is called a **probability mass function**

<!-- ## Simulating outcomes in R -->

<!-- We can **simulate** coin flips in R using the `sample` function -->

<!-- \tiny -->
<!-- ```{r} -->
<!-- possible_events <- c("H", "T") -->
<!-- coin_flips <- sample(possible_events, size = 4, replace = TRUE) -->
<!-- coin_flips -->
<!-- ``` -->

<!-- ## Simulating outcomes in R -->

<!-- We can do this simulation experiment more than once and count the number of observed heads each time. For example, the output below show the results of 100 experiments (of four coin flips) and the number of times each value of heads was observed. -->
<!-- \vspace{5mm} -->

<!-- ```{r, echo = F} -->
<!-- library(kableExtra) -->
<!-- number_of_experiments <- 100 -->
<!-- coin_flip_results <- tibble() -->

<!-- set.seed(86) -->
<!-- for(i in 1:number_of_experiments){ -->
<!--   coin_flips <- sample(possible_events, size = 4, replace = TRUE) -->
<!--   this_result <- tibble(experiment = i, flip = 1:4, outcome = coin_flips) -->
<!--   coin_flip_results <- bind_rows(coin_flip_results, this_result) -->
<!-- } -->

<!-- samp_dist <- coin_flip_results %>%  -->
<!--   group_by(experiment) %>%  -->
<!--   summarise(heads = sum(outcome=="H")) %>%  -->
<!--   group_by(heads) %>%   -->
<!--   tally()  -->

<!-- samp_dist %>% kable() -->

<!-- ``` -->

<!-- ## The sampling distribution -->

<!-- ```{r, echo = FALSE} -->
<!-- samp_dist %>%  -->
<!--   mutate(probability = n/sum(n)) %>%  -->
<!--   kable() -->
<!-- ``` -->

<!-- A **sampling distribution** is a probability distribution for a statistic based on repeated samples. Here, our random-sample-based statistic is the number of heads in four coin flips.  -->


<!-- ## The sampling distribution -->

<!-- Each coin flip (or any experiment with only two outcomes) is called a **Bernoulli trial**. A series of two or more independent Bernoulli trials constitutes a **binomial experiment**: -->

<!-- 1. Each trial can have only two outcomes, often called success and failure. -->
<!-- 2. The experiment has a specified number of trials, usually denoted by $n$. -->
<!-- 3. The probability of a success is usually denoted by $p$ and the probability of a failure by $q$; -->
<!-- thus, the sum of $p$ and $q$ is equal to 1 -->
<!-- 4. The trials are independent of one another.  -->
<!-- 5. The random variable is defined as the number of successes observed in the $n$ trials. -->

<!-- The sampling distribution of the coin flips example is a **Binomial distribution** with $n = 4$ and $p = 0.5$ -->

<!-- ## The expected value of a random variable -->

<!-- - In the first week, we discussed summary measures of data (measures of central tendency, spread, range) -->
<!-- - These types of measures are also useful to summarize sampling distributions of random variables.  -->

<!-- Intuitively, on average, we would expect to see about two heads and two tails in any sequence of four trials. Imagine that we were to replicate the four tosses many, many times and take note of the number of heads in each of the series of four tosses. After many, many series, we could find the mean number of heads across the large number of series; it would seem reasonable to expect that the mean should be quite close to two. This is the **expected value**.  -->

<!-- ## The expected value of a random variable -->

<!-- For a discrete random variable, $X$, with a known probability distribution $P(X_i)$ and where $X_i$ is the $i$th outcome in the set of $k$ simple events: -->

<!-- \footnotesize -->
<!-- $$ -->
<!-- E(X)=X_{1} \times P\left(X_{1}\right)+X_{2} \times P\left(X_{2}\right)+\ldots+X_{k} \times P\left(X_{k}\right)=\sum_{i=1}^{k} X_{i} \times P\left(X_{i}\right)=\mu -->
<!-- $$ -->

<!-- \normalsize -->
<!-- The expected value is a weighted mean of all the possible values of the RV, weighted by their probabilities. It is given the symbol $\mu$.  -->

<!-- Calculate the expected value for the number of heads in four coin flips.  -->

<!-- ## The variance of a random variable -->

<!-- In week 1 we defined the variance is the average of the squared deviations from the mean. We can use the definition of expected value to derive the variance, given the probability distribution  -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \sigma^{2} &=E\left[(X-\mu)^{2}\right]\\ -->
<!-- &=\left[X_{1}-E(X)\right]^{2} \times P\left(X_{1}\right)+\ldots+\left[X_{k}-E(X)\right]^{2} \times P\left(X_{k}\right) \\ -->
<!-- &=\sum_{i=1}^{k}\left[X_{i}-E(X)\right]^{2} \times P\left(X_{i}\right) -->
<!-- \end{aligned} -->
<!-- $$ -->


<!-- Calculate the variance for the number of heads in four coin flips.  -->

<!-- ## Summary -->

<!-- - If we know the probability distribution of a discrete random variable, we know the mean and variance of the random variable, and hence, the standard deviation of the random variable.  -->
<!-- - Thus, we can make predictions about where the values should center and how spread out they should be.  -->
<!-- - If the random variable X is a continuous variable, then the idea is the same, but the sums $\sum$ need to be replaced with integrals $\int$ and we would need some calculus.  -->

## Probabilities as areas

```{r, echo = FALSE}
ggplot(tibble(heads = 0:4, probability = c(1/16, 4/16, 6/16, 4/16, 1/16)), aes(x = heads, y = probability))+geom_bar(stat = "identity")
```

## Probabilities as areas

- To calculate probabilities, can sum up the area of the rectangles
- E.g. $P(X\geq3)$ would be the sum of the right two rectangles
- What is $P(1 \leq X \leq3)$?
- What is $P(1 \leq X <3)$?

## Continuous random variables and probability distributions

- So far we have talked about a discrete RV
- But a lot of our RVs of interest are continuous (e.g. height)
- Can think about in the same way (defining probability distributions, expected values, etc) 
- Instead of having a table of values making up the probability distribution (or pmf), we have a mathematically defined function 
- A probability distribution for a continuous RV is called a **probability density function**

## A continuous probability distribution is just a histogram with infinitely small bins

```{r, echo = FALSE}
set.seed(76)
heights <- tibble(height = rnorm(18000, mean = 170, sd = 8))
heights %>% 
  ggplot(aes(height)) + 
  geom_histogram(binwidth = 5, color = "firebrick4", fill = "steelblue4")+
  ggtitle("Adult heights")
```

##

```{r, echo = FALSE}
heights %>% 
  ggplot(aes(height)) + 
  geom_histogram(binwidth = 3, color = "firebrick4", fill = "steelblue4")+
  ggtitle("Adult heights")
```

##

```{r, echo = FALSE}
heights %>% 
  ggplot(aes(height)) + 
  geom_histogram(binwidth = 2, color = "firebrick4", fill = "steelblue4", aes(y = ..density..))+
  ggtitle("Adult heights")
```

## Probability density function

```{r, echo = FALSE}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 170,
                  sd = 8
                ),
                fill = "steelblue4", color = "firebrick4") +
  xlim(c(140,200))+
  ylab("density") + 
  ggtitle("Adult heights")
```



## Probabilities as areas

```{r, echo = FALSE}
ggplot() +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    color = "firebrick4",
    alpha = .3,
    args = list(
                  mean = 170,
                  sd = 8
                )
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    args = list(
                  mean = 170,
                  sd = 8
                ),
    xlim = c(185, 200)
  ) +
  ylab("Density")+
  scale_x_continuous(limits = c(140, 200), breaks = seq(140, 200, by = 15))+
    ggtitle("Adult heights")
```

## Probabilities as areas

```{r, echo = FALSE, fig.height=4}
ggplot() +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    color = "firebrick4",
    alpha = .3,
    args = list(
                  mean = 170,
                  sd = 8
                )
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    args = list(
                  mean = 170,
                  sd = 8
                ),
    xlim = c(185, 200)
  ) +
  ylab("Density")+
  scale_x_continuous(limits = c(140, 200), breaks = seq(140, 200, by = 15))+
    ggtitle("Adult heights")
```

- The probability that height is greater than 185cm i.e. $P(X > 185)$
- Like summing up very tiny histogram bins above a certain point

## Probability as areas

Important notes

- The sum of the whole area under the curve is equal to 1 (because we know all probabilities have to sum to one)
- A value is either greater than or less than/equal to a number
- So can express probabilities as the complement e.g. $P(X> 185) = 1 - P(X \leq 185)$ 


```{r, echo = FALSE, fig.height=3}
ggplot() +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    color = "firebrick4",
    alpha = .3,
    args = list(
                  mean = 170,
                  sd = 8
                )
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    args = list(
                  mean = 170,
                  sd = 8
                ),
    xlim = c(185, 200)
  ) +
  ylab("Density")+
  scale_x_continuous(limits = c(140, 200), breaks = seq(140, 200, by = 15))+
    ggtitle("Adult heights")
```

# The normal distribution


## The normal distribution

- One of the most important continuous probability distributions 
- Is described by the formula

$$
f(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2} \frac{(x-\mu)^{2}}{\sigma^{2}}}
$$

- The shape is determined by two **parameters**, $\mu$ and $\sigma$
- If we were to plot $f(x)$ as a function of $x$, we would obtain a normal distribution that would be centered at whatever value of $\mu$ we specified, and it would have a standard deviation equal to $\sigma$. 

##

```{r, echo = FALSE}
ggplot() +
  stat_function(
    fun = dnorm,
    geom = "area",
    aes(fill = "mu = 170, sd = 8"),
    color = "black",
    alpha = .3,
    args = list(
                  mean = 170,
                  sd = 8
                ))+
  stat_function(
    fun = dnorm,
    geom = "area",
    aes(fill = "mu = 170, sd = 4"),
    color = "black",
    alpha = .2,
    args = list(
                  mean = 170,
                  sd = 4
                ))+
    stat_function(
    fun = dnorm,
    geom = "area",
    aes(fill = "mu = 185, sd = 4"),
    color = "black",
    alpha = .2,
    args = list(
                  mean = 185,
                  sd = 4
                ))+
  scale_x_continuous(limits = c(140, 210), breaks = seq(140, 200, by = 15))+
  scale_fill_brewer(name="", palette = "Set1")
```

## The normal distribution

- Many variables naturally resemble the normal distribution (or can be transformed to be so)
- Height, weight, intelligence...
- Strong relationships with other distributions 
- Many sample statistics are normally distributed (more later)

## The standard normal distribution

A special case of the normal distribution with $\mu = 0$ and $\sigma = 1$.

```{r, echo = FALSE, fig.height = 5}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 0,
                  sd = 1
                ),
                fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) +
  scale_x_continuous(limits = c(-4,4), breaks = -4:4)+
  ylab("density") 
```

## The standard normal distribution

- ~68% of area within 1 standard deviation


```{r, echo = FALSE, fig.height = 5}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 0,
                  sd = 1
                ),
                fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) +
  scale_x_continuous(limits = c(-4,4), breaks = -4:4)+
  ylab("density") +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.5,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(-1, 1)
  )
```


## The standard normal distribution

- ~95% of the area within 2 standard deviations

```{r, echo = FALSE, fig.height = 5}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 0,
                  sd = 1
                ),
                fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) +
  scale_x_continuous(limits = c(-4,4), breaks = -4:4)+
  ylab("density")+
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.5,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(-2, 2)
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.8,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(-1, 1)
  )
```

## Any normal distribution can be transformed into the standard normal

Say $X$ is normally distributed with mean $\mu$ and variance $\sigma^2$. We can write this as 

$$
X \sim N(\mu, \sigma^2)
$$
We can transform $X$ using the **$z$-transformation**

$$
\frac{X - \mu}{\sigma}
$$
Call this transformed version $Z$ i.e. $Z = \frac{X - \mu}{\sigma}$. Then

$$
Z \sim N(0,1)
$$
we can refer to the transformed version as **Z-scores**. 

<!-- ## Steps -->

<!-- $$ -->
<!-- X \sim N(\mu, \sigma^2) -->
<!-- $$ -->
<!-- $$ -->
<!-- X - \mu \sim N(0, \sigma^2) -->
<!-- $$ -->
<!-- because $E(X+c) = E(X)+c$ -->

<!-- $$ -->
<!-- \frac{X - \mu}{\sigma} \sim N(0, 1) -->
<!-- $$ -->

<!-- because $Var(cX) = c^2Var(X)$ -->

## Z-scores 

- Z-scores tell you the number of standard deviations by which the value of a raw score is above or below the mean value.
- In the heights example, the mean $\mu = 170$ and standard deviation $\sigma = 8$.

Rohan is 180cm. What is his Z-score?

$$
Z = \frac{180-170}{8} = 1.25
$$
So Rohan is 1.25 standard deviations above the mean height. 

- Monica is 168cm, so her Z-score is -0.25. So she is 0.25 standard deviations below the mean height. 

## Z-scores

```{r, echo = FALSE, fig.height = 3}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 0,
                  sd = 1
                ),
                fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) +
  scale_x_continuous(limits = c(-4,4), breaks = -4:4)+
  ylab("density")+
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.5,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(-1.25, 1.25)
  ) 
```

- Recall that ~95% of the area was within 2 standard deviations. 
- We can flip this and ask what proportion of the area of a standard normal is within 1.25 standard deviations?
- The answer is ~79%

## Z-scores 

- ~79% of the area is within 1.25 standard deviations
- Alternative interpretation: If we randomly draw a value from a standard normal distribution, the value will be between [-1.25,1.25] 79% of the time. 
- Conversely, 21% of values will fall outside that range

```{r, echo = FALSE, fig.height = 3}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 0,
                  sd = 1
                ),
                fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) +
  scale_x_continuous(limits = c(-4,4), breaks = -4:4)+
  ylab("density")+
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.5,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(-4, -1.25)
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.5,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(1.25, 4)
  )
```



<!-- ## Z-scores -->

<!-- - If the Z-score is small (i.e. the absolute value is close to zero), then the observed value is likely to come from that distribution -->
<!-- - E.g. Monica's Z-score was -0.25 -->
<!--     + 40% of the total area falls outside the [-0.25, 0.25] range -->
<!--     + So it would be quite likely to observe outside this range, just by chance -->


<!-- ```{r, echo = FALSE, fig.height = 3} -->
<!-- ggplot() + -->
<!--   stat_function(fun = dnorm, -->
<!--                 geom = "area", -->
<!--                 args = list( -->
<!--                   mean = 0, -->
<!--                   sd = 1 -->
<!--                 ), -->
<!--                 fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) + -->
<!--   scale_x_continuous(limits = c(-4,4), breaks = -4:4)+ -->
<!--   ylab("density")+ -->
<!--   stat_function( -->
<!--     fun = dnorm, -->
<!--     geom = "area", -->
<!--     fill = "steelblue4", -->
<!--     alpha = 0.5, -->
<!--     args = list( -->
<!--                   mean = 0, -->
<!--                   sd = 1 -->
<!--                 ), -->
<!--     xlim = c(-4, -0.25) -->
<!--   ) + -->
<!--   stat_function( -->
<!--     fun = dnorm, -->
<!--     geom = "area", -->
<!--     fill = "steelblue4", -->
<!--     alpha = 0.5, -->
<!--     args = list( -->
<!--                   mean = 0, -->
<!--                   sd = 1 -->
<!--                 ), -->
<!--     xlim = c(0.25, 4) -->
<!--   ) -->
<!-- ``` -->

<!-- - In contrast it would be less likely to observe something outside the range of Rohan's Z-score [-1.25, 1.25] -->
<!-- - If we observed an even bigger Z-score, it would be even less likely -->


<!-- ## Example simulation in R -->

<!-- - We can generate random draws from a normal distribution in R using the `rnorm` function.  -->
<!-- - For example, the following code generates 1000 observations from a standard normal and then calculates what proportion are above a Z-score of 1.25 or below -1.25.  -->

<!-- \tiny -->
<!-- ```{r} -->
<!-- set.seed(1889) # makes sure the random numbers generated are the same each time -->
<!-- # rnorm allows you to simulate values from a normal distribution -->
<!-- z_scores <- rnorm(n = 1000, mean = 0, sd = 1)  -->
<!-- z_scores[1:10] # show the first ten draws -->
<!-- (sum(z_scores< -1.25) + sum(z_scores> 1.25))/1000 -->
<!-- ``` -->


<!-- ## Z-scores as critical values and hypothesis testing -->

<!-- We can formalize these ideas through **hypothesis testing** -->

<!-- - Say we observe a person's height, and it's equal to 196cm. -->
<!-- - We also know that the underlying distribution has a mean of 170cm and a standard deviation of 8cm. -->
<!-- - We want to test to see whether this person's height is **significantly different** from the underlying distribution's mean  -->

<!-- The **null hypothesis** is  -->

<!-- $$ -->
<!-- H_0 -->
<!-- $$ -->

## Finding areas under the curve using R

You can find the area under a normal curve in R using the `pnorm` function, which returns the cumulative probability from $-\infty$ to the value supplied to the `pnorm` function. 

For example, find the area below z = -1.25:

\tiny
```{r}
pnorm(-1.25)
```

\normalsize

This corresponds to

```{r, echo = FALSE, fig.height = 3}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 0,
                  sd = 1
                ),
                fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) +
  scale_x_continuous(limits = c(-4,4), breaks = -4:4)+
  ylab("density")+
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.5,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(-4, -1.25)
  ) 
```



## Finding areas under the curve using R

To find the area above 1.25:

\tiny
```{r}
1-pnorm(1.25)
# or 
# pnorm(1.25, lower.tail = FALSE)
```

\normalsize

This corresponds to

```{r, echo = FALSE, fig.height = 3}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 0,
                  sd = 1
                ),
                fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) +
  scale_x_continuous(limits = c(-4,4), breaks = -4:4)+
  ylab("density")+
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.5,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(1.25, 4)
  ) 
```



## Finding areas under the curve using R

To find the area between -1.25 and 1.25

\tiny
```{r}
pnorm(1.25) - pnorm(-1.25)
```

\normalsize

This corresponds to

```{r, echo = FALSE, fig.height = 3}
ggplot() +
  stat_function(fun = dnorm,
                geom = "area",
                args = list(
                  mean = 0,
                  sd = 1
                ),
                fill = NA, color = "firebrick4", alpha = 0.4, lwd = 1.2) +
  scale_x_continuous(limits = c(-4,4), breaks = -4:4)+
  ylab("density")+
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue4",
    alpha = 0.5,
    args = list(
                  mean = 0,
                  sd = 1
                ),
    xlim = c(-1.25, 1.25)
  ) 
```


# Sampling distributions and the central limit theorem

## Why do we care about the Normal distribution so much

- It's just one distribution
- It tends to over-simplify the real distribution of outcomes/variables

BUT it turns out that the distribution of summary statistics that we're interested in (e.g. means) tend towards being Normal 

- this is important for regression and inference

## Sampling distributions 

A **sampling distribution** is a probability distribution for a statistic based on repeated samples.

Say we are interested in taking a random sample of people's heights, $X$ and calculating the mean height for that sample. So our statistic of interest is the mean height, $\bar{X}$.


## Randomly sampling heights


We first take a random sample of 12 people and get the following heights

\tiny
```{r, echo = FALSE}
heights_1 <- rnorm(12, mean = 170, sd = 8)
round(heights_1, 1)
```
\normalsize
The observed mean height of this sample is `r round(mean(heights_1),1)`.

We take a random sample of another 12 people and get the following heights:

\tiny
```{r, echo = FALSE}
heights_1 <- rnorm(12, mean = 170, sd = 8)
round(heights_1, 1)
```

\normalsize
The observed mean height of this sample is `r round(mean(heights_1),1)`.

## Randomly sampling heights

Say that I keep doing this process again and again and again, and end up with 100 observations of mean height. I can plot a histogram of these means:

```{r, echo = FALSE, fig.height = 5}

N <- 100 
heights_0 <- tibble()

for(i in 1:N){
  these_heights = tibble(sample = i, heights = rnorm(12, 170, 8))
  heights_0 <- bind_rows(heights_0, these_heights)
}

heights_0 %>% 
  group_by(sample) %>% 
  summarise(mean_height = mean(heights)) %>% 
  ggplot(aes(mean_height)) + geom_histogram(color = "navy", fill = "lightblue")

```

## Randomly sampling heights

Okay, what if I take 1000 samples. I plot a histogram of the means again

```{r, echo = FALSE, fig.height = 5}

N <- 1000
heights <- tibble()

for(i in 1:N){
  these_heights = tibble(sample = i, heights = rnorm(12, 170, 8))
  heights <- bind_rows(heights, these_heights)
}

heights %>% 
  group_by(sample) %>% 
  summarise(mean_height = mean(heights)) %>% 
  ggplot(aes(mean_height)) + geom_histogram(color = "navy", fill = "lightblue")

```
What do you notice?

## Randomly sampling heights

What about 10,000 samples?

```{r, echo = FALSE, fig.height = 5}

N <- 10000
heights_2 <- tibble()

for(i in 1:N){
  these_heights = tibble(sample = i, heights = rnorm(12, 170, 8))
  heights_2 <- bind_rows(heights_2, these_heights)
}

heights_2 %>% 
  group_by(sample) %>% 
  summarise(mean_height = mean(heights)) %>% 
  ggplot(aes(mean_height)) + geom_histogram(color = "navy", fill = "lightblue")

```

## The central limit theorem

The distribution of the sum (or mean) of a set of independent random variables will **tend towards** a normal distribution. 

- "tend towards" means as the number of observations of the sum or mean gets larger, the distribution will become more normal
- The central limit theorem holds even if the original variables themselves are not normally distributed. 


## The central limit theorem

For a random variable $X$ with $E(X) = \mu$ and $Var(X) = \sigma^2$, the central limit theorem results in the following distribution for the mean $\bar{X}$:

$$
\bar{X} \sim N(\mu, \frac{\sigma^2}{n})
$$

What does this mean?

- The mean $\bar{X}$ will be centered at the same value as $X$
- The variance of $\bar{X}$ depends on the variance of the original random variable $X$ and also the number of samples of the mean we have, $n$.

The quantity $\frac{\sigma}{\sqrt n}$ is also called the **standard error of the mean**.

## Sampling heights

Before, we were repeatedly taking a random sample of 12 heights. 

When we did this 100 times, we had 100 observations of $\bar{X}$. The mean was `r round(mean(heights_0$heights))` and the standard error of the mean was `r round(sd(heights_0$heights), 1)/100`.

When we did this 10000 times, we had 100 observations of $\bar{X}$. The mean was `r round(mean(heights_2$heights))` and the standard error of the mean was `r round(sd(heights_2$heights), 1)/10000`.


## Summary

- Probability concepts
- Probability distributions
- Probabilities as areas
- The normal distribution
- The standard normal distribution and Z scores
- Sampling distributions
- The central limit theorem
- Standard error of the mean

## Lab

- Finish off tidyverse bit
- Calculating random variables and probabilities
- Intro to ggplot

